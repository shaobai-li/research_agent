{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import json\n",
    "from system_prompts import SYSTEM_PROMPT_FIRST_SEARCH, SYSTEM_PROMPT_FIRST_SUMMARY, SYSTEM_PROMPT_REFLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66753838",
   "metadata": {},
   "source": [
    "### 加载环境变量和预定义函数及数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29304bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class Search:\n",
    "    url: str = \"\"\n",
    "    content: str = \"\"\n",
    "    \n",
    "@dataclass\n",
    "class Research:\n",
    "    search_history: List[Search] = field(default_factory=list)\n",
    "    latest_summary: str = \"\"\n",
    "    reflection_iteration: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Paragraph:\n",
    "    title: str = \"\"\n",
    "    content: str = \"\"\n",
    "    research: Research = field(default_factory=Research)\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    report_title: str = \"\"\n",
    "    paragraphs: List[Paragraph] = field(default_factory=list)\n",
    "\n",
    "\n",
    "def clean_json_tags(json_str):\n",
    "    return json_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "def update_state_with_search_results(search_results, idx_paragraph, state):\n",
    "\n",
    "    for search_result in search_results[\"results\"]:\n",
    "        search = Search(url=search_result[\"url\"], content=search_result[\"content\"])\n",
    "        state.paragraphs[idx_paragraph].research.search_history.append(search)\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa26a39",
   "metadata": {},
   "source": [
    "### 初始化状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da54c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = State()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52cee8",
   "metadata": {},
   "source": [
    "### 报告大纲智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43bc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportStructureAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def get_system_prompt(self):        \n",
    "        output_schema_report_structure = {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"title\": {\"type\": \"string\"},\n",
    "                        \"content\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "        example_output_report_structure = \"\"\"\n",
    "```json\n",
    "[\n",
    "{\n",
    "    \"title\": \"智能体的定义\",\n",
    "    \"content\": \"智能体是具有感知、决策、执行和学习能力的自主系统。\"\n",
    "},\n",
    "{\n",
    "    \"title\": \"智能体的分类\",\n",
    "    \"content\": \"智能体可以分为以下几类：\"\n",
    "}\n",
    "]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "        system_prompt = f\"\"\"\n",
    "你是一个深度调研助手。针对一个查询任务，请规划一份报告的结构以及应包含的段落内容。\n",
    "请确保段落的排列顺序合理。\n",
    "结构制定完成后，你将获得工具来分别为每个部分进行网页搜索和反思。\n",
    "请按照以下 JSON 格式定义的格式输出结果：\n",
    "\n",
    "<OUTPUT JSON SCHEMA>\n",
    "{json.dumps(output_schema_report_structure, indent=2)}\n",
    "</OUTPUT JSON SCHEMA>\n",
    "\n",
    "标题（title）和内容（content）属性将用于后续的深入研究。\n",
    "请确保输出是一个符合上述 JSON 格式定义的 JSON 对象。\n",
    "只返回 JSON 对象，不要附加任何解释或额外文本。\n",
    "\n",
    "<EXAMPLE OUTPUT>\n",
    "{example_output_report_structure}\n",
    "</EXAMPLE OUTPUT>\n",
    "\"\"\"\n",
    "\n",
    "        return system_prompt\n",
    "        \n",
    "    def run(self, input):\n",
    "        response = self.llm.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.get_system_prompt()},\n",
    "                {\"role\": \"user\", \"content\": input}\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "        report_structure = json.loads(clean_json_tags(response.choices[0].message.content))\n",
    "\n",
    "        return report_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec04e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "报告大纲：\n",
      "第0个段落标题: Web3.0的定义和背景\n",
      "第0个段落内容: 介绍Web3.0的概念及其起源，区分其与Web1.0和Web2.0的差异，强调去中心化和用户自主权的重要性。\n",
      "第1个段落标题: Web3.0的关键技术\n",
      "第1个段落内容: 探讨区块链、智能合约、去中心化应用（DApps）、分布式共识机制等Web3.0的核心技术及其运作方式。\n",
      "第2个段落标题: Web3.0的开发工具和框架\n",
      "第2个段落内容: 列举支持Web3.0开发的主要工具和框架，如Truffle、React、Web3.js等，并说明其功能和使用场景。\n",
      "第3个段落标题: 开发Web3.0应用的流程\n",
      "第3个段落内容: 介绍如何从概念到实现进行Web3.0应用开发的基本流程，包括需求分析、智能合约编写、前后端集成等步骤。\n",
      "第4个段落标题: Web3.0的安全性和隐私\n",
      "第4个段落内容: 分析Web3.0在安全性和隐私保护方面的挑战和对策，探讨常见的安全漏洞及如何防范。\n",
      "第5个段落标题: Web3.0的行业应用场景\n",
      "第5个段落内容: 展示Web3.0技术在金融、供应链、社交媒体、数字身份等行业的应用案例，分析其带来的创新和优势。\n",
      "第6个段落标题: Web3.0开发的挑战与机遇\n",
      "第6个段落内容: 讨论目前Web3.0在开发过程中面临的技术、法律、用户体验等挑战，以及其带来的市场机遇和未来趋势。\n",
      "第7个段落标题: 如何学习和加入Web3.0社区\n",
      "第7个段落内容: 提供学习Web3.0开发的资源和机会，介绍相关的社区、论坛和培训课程，帮助开发者快速上手和深度参与。\n"
     ]
    }
   ],
   "source": [
    "research_structure_agent = ReportStructureAgent()\n",
    "\n",
    "research_input = \"请帮我调研一下web3.0的开发\"\n",
    "\n",
    "report_structure = research_structure_agent.run(research_input)\n",
    "for paragraph in report_structure:\n",
    "    STATE.paragraphs.append(Paragraph(title=paragraph[\"title\"], content=paragraph[\"content\"]))\n",
    "\n",
    "print(\"报告大纲：\")\n",
    "for i, paragraph in enumerate(STATE.paragraphs):\n",
    "    print(f\"第{i}个段落标题: {paragraph.title}\") \n",
    "    print(f\"第{i}个段落内容: {paragraph.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81090a",
   "metadata": {},
   "source": [
    "### 搜索智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97bcb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tavily\n",
    "\n",
    "class SearchAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.search_tool = tavily.TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "    )\n",
    "\n",
    "    def get_system_prompt(self):\n",
    "        \n",
    "\n",
    "        input_schema_first_search = {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"title\": {\"type\": \"string\"},\n",
    "                        \"content\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "\n",
    "        output_schema_first_search = {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\"type\": \"string\"},\n",
    "                        \"reasoning\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "\n",
    "\n",
    "        system_prompt = f\"\"\"\n",
    "你是一个深度研究助手。你将会被提供一段报告中的段落、它的标题以及期望内容，格式如下的 JSON 格式定义：\n",
    "\n",
    "<INPUT JSON SCHEMA>\n",
    "{json.dumps(input_schema_first_search, indent=2)}\n",
    "</INPUT JSON SCHEMA>\n",
    "\n",
    "你可以使用一个网页搜索工具，该工具接受一个 search_query 作为参数。\n",
    "你的任务是对该主题进行思考，并提供一个最优的网页搜索查询（请使用中文），以丰富你当前的知识。\n",
    "请按照以下的 JSON 格式定义来格式化你的输出：\n",
    "\n",
    "<OUTPUT JSON SCHEMA>\n",
    "{json.dumps(output_schema_first_search, indent=2)}\n",
    "</OUTPUT JSON SCHEMA>\n",
    "\n",
    "请确保你的输出是一个符合上述输出 JSON 架构定义的 JSON 对象。\n",
    "只返回 JSON 对象，不要附加任何解释或其他文本。\n",
    "\"\"\"\n",
    "\n",
    "        return system_prompt\n",
    "\n",
    "    def run(self, input):                \n",
    "        response = self.llm.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":self.get_system_prompt()},\n",
    "                {\"role\":\"user\",\"content\":json.dumps(input)}\n",
    "                ],\n",
    "            temperature=1\n",
    "            )\n",
    "        \n",
    "        search_arguments = json.loads(clean_json_tags(response.choices[0].message.content))\n",
    "\n",
    "        return search_arguments\n",
    "    \n",
    "    def search(self, search_query, include_raw_content=True, max_results=5):\n",
    "        search_results = self.search_tool.search(\n",
    "                query=search_query,\n",
    "                include_raw_content=include_raw_content,\n",
    "                max_results=max_results\n",
    "                )\n",
    "        return search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b43f7967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web3.0 定义 起源 与 Web1.0 Web2.0 的区别 去中心化 用户自主权\n",
      "https://www.cnblogs.com/listen80/p/18304967\n",
      "Web1.0：虽然内容由少数人创建，但平台的中心化程度相对较低。 · Web2.0：高度中心化，平台拥有绝对的控制权和规则制定权。 · Web3.0：去中心化的架构，减少了对\n"
     ]
    }
   ],
   "source": [
    "search_agent = SearchAgent()\n",
    "\n",
    "search_input = {\n",
    "    \"title\": STATE.paragraphs[0].title,\n",
    "    \"content\": STATE.paragraphs[0].content\n",
    "}\n",
    "\n",
    "\n",
    "search_arguments = search_agent.run(search_input)\n",
    "print(search_arguments[\"search_query\"])\n",
    "\n",
    "search_results = search_agent.search(search_arguments[\"search_query\"])\n",
    "\n",
    "STATE = update_state_with_search_results(search_results, 0, STATE)\n",
    "\n",
    "for search in STATE.paragraphs[0].research.search_history:\n",
    "    print(search.url)\n",
    "    print(search.content)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25635003",
   "metadata": {},
   "source": [
    "### 段落输出智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23621d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritingAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def get_system_prompt(self):\n",
    "                \n",
    "        input_schema_first_summary = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"content\": {\"type\": \"string\"},\n",
    "                \"search_query\": {\"type\": \"string\"},\n",
    "                \"search_results\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "        output_schema_first_summary ={\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paragraph_latest_state\": {\"type\": \"string\"}\n",
    "                }\n",
    "        }\n",
    "\n",
    "        system_prompt = f\"\"\"\n",
    "你是一个深度研究助手。你将会获得一个搜索查询、一组搜索结果，以及一份报告中的段落（需要你撰写），格式如下所示的 JSON 架构定义：\n",
    "\n",
    "<INPUT JSON SCHEMA>\n",
    "{json.dumps(input_schema_first_summary, indent=2)}\n",
    "</INPUT JSON SCHEMA>\n",
    "\n",
    "你的任务是：作为研究者，使用这些搜索结果来撰写该段落，使其内容与段落主题一致，并具有良好的结构，可以直接纳入到报告中。\n",
    "请将输出格式化为以下 JSON 格式定义：\n",
    "\n",
    "<OUTPUT JSON SCHEMA>\n",
    "{json.dumps(output_schema_first_summary, indent=2)}\n",
    "</OUTPUT JSON SCHEMA>\n",
    "\n",
    "确保输出是一个符合上述输出 JSON 格式定义的 JSON 对象。\n",
    "只返回 JSON 对象，不要附加解释或额外文本。\n",
    "\"\"\"\n",
    "        return system_prompt\n",
    "\n",
    "    def run(self, input):\n",
    "                \n",
    "        response = self.llm.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":self.get_system_prompt()},\n",
    "                {\"role\":\"user\",\"content\":json.dumps(input)}\n",
    "                ],\n",
    "            temperature=1\n",
    "            )\n",
    "        \n",
    "        paragraph = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        return paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4e651b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `deepseek-reasoner` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m writing_agent \u001b[38;5;241m=\u001b[39m WritingAgent()\n\u001b[0;32m      3\u001b[0m input_search_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATE\u001b[38;5;241m.\u001b[39mparagraphs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtitle,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATE\u001b[38;5;241m.\u001b[39mparagraphs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_query\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_results\u001b[39m\u001b[38;5;124m\"\u001b[39m: [ result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_content\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m search_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_content\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      8\u001b[0m }\n\u001b[1;32m---> 10\u001b[0m paragraph \u001b[38;5;241m=\u001b[39m writing_agent\u001b[38;5;241m.\u001b[39mrun(input_search_results)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(paragraph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparagraph_latest_state\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[38], line 48\u001b[0m, in \u001b[0;36mWritingAgent.run\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     49\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-reasoner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     51\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_system_prompt()},\n\u001b[0;32m     52\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28minput\u001b[39m)}\n\u001b[0;32m     53\u001b[0m             ],\n\u001b[0;32m     54\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     55\u001b[0m         )\n\u001b[0;32m     57\u001b[0m     paragraph \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m paragraph\n",
      "File \u001b[1;32md:\\prog\\miniconda3\\envs\\agent\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\prog\\miniconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    912\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    913\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    916\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    917\u001b[0m             {\n\u001b[0;32m    918\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    919\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    920\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    921\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    922\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    923\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    924\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    925\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    926\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    927\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    928\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    929\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    930\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    931\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    932\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    933\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    934\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    935\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    936\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    937\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    938\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    939\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    940\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    941\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    942\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    943\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    944\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    945\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    946\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    947\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    948\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m    949\u001b[0m             },\n\u001b[0;32m    950\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m    951\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m    952\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m    953\u001b[0m         ),\n\u001b[0;32m    954\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    955\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    956\u001b[0m         ),\n\u001b[0;32m    957\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    958\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    959\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    960\u001b[0m     )\n",
      "File \u001b[1;32md:\\prog\\miniconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32md:\\prog\\miniconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    920\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    921\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    922\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    923\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    924\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    925\u001b[0m )\n",
      "File \u001b[1;32md:\\prog\\miniconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py:1023\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1022\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1026\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1027\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1032\u001b[0m )\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `deepseek-reasoner` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "writing_agent = WritingAgent()\n",
    "\n",
    "input_search_results = {\n",
    "    \"title\": STATE.paragraphs[0].title,\n",
    "    \"content\": STATE.paragraphs[0].content,\n",
    "    \"search_query\": search_arguments[\"search_query\"],\n",
    "    \"search_results\": [ result[\"raw_content\"] for result in search_results[\"results\"] if result[\"raw_content\"]]\n",
    "}\n",
    "\n",
    "paragraph = writing_agent.run(input_search_results)\n",
    "\n",
    "print(paragraph[\"paragraph_latest_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e334513",
   "metadata": {},
   "source": [
    "### 反思智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json_reflection = {\n",
    "    \"title\": STATE.paragraphs[0].title,\n",
    "    \"content\": STATE.paragraphs[0].content,\n",
    "    \"paragraph_latest_state\": paragraph_output[\"paragraph_latest_state\"]\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":SYSTEM_PROMPT_REFLECTION},\n",
    "        {\"role\":\"user\",\"content\":json.dumps(input_json_reflection)}\n",
    "        ],\n",
    "    temperature=1\n",
    "    )\n",
    "    \n",
    "reflection_output = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(reflection_output[\"search_query\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
